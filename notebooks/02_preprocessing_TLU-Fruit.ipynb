# %% [markdown]
# # 02. TLU-Fruit Dataset Preprocessing
#
# This notebook handles the preprocessing of the TLU-Fruit dataset.
# The key steps include:
# 1. Loading raw annotations.
# 2. Text preprocessing (tokenization of questions and answers using a BERT tokenizer).
# 3. Image preprocessing strategy:
#     - Processing original images (resizing, normalization).
#     - Handling paths for segmented images (assumed to be pre-generated or placeholder).
#     - Handling paths for cropped images (assumed to be pre-generated or placeholder).
# 4. Splitting the dataset into training, validation, and test sets.
# 5. Saving the processed data.

# %% [code]
import json
import os
import shutil # For file operations like copying
import pandas as pd
import numpy as np
from PIL import Image
from sklearn.model_selection import train_test_split
from transformers import BertTokenizerFast # Using BertTokenizerFast for speed
import torch
from torchvision import transforms
from tqdm.auto import tqdm # For progress bars

# Plot settings (optional, for any quick checks)
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("whitegrid")

# %% [markdown]
# ## 1. Define Paths and Configuration

# %% [code]
# --- Configuration ---
BERT_MODEL_NAME = 'bert-base-uncased' # Choose your BERT model
MAX_TOKEN_LENGTH = 128 # Max sequence length for BERT
IMAGE_SIZE = (224, 224) # Target image size for ViT/CNN
TEST_SET_RATIO = 0.125 # 1 part for test
VALIDATION_SET_RATIO = 0.125 # 1 part for validation (relative to the remaining 7 parts)
RANDOM_STATE = 42 # For reproducible splits

# --- Input Paths (Raw Data) ---
BASE_DATA_PATH_RAW = os.path.join("..", "data", "raw", "TLU-Fruit")
RAW_ANNOTATIONS_FILE = os.path.join(BASE_DATA_PATH_RAW, "annotations_tlu_fruit.json") # Adjust if your filename is different
RAW_IMAGES_DIR = os.path.join(BASE_DATA_PATH_RAW, "images")

# --- Output Paths (Processed and Interim Data) ---
BASE_DATA_PATH_PROCESSED = os.path.join("..", "data", "processed", "TLU-Fruit")
BASE_DATA_PATH_INTERIM = os.path.join("..", "data", "interim", "TLU-Fruit")

PROCESSED_IMAGES_ORIGINAL_DIR = os.path.join(BASE_DATA_PATH_INTERIM, "images_original_processed")
PROCESSED_IMAGES_SEGMENTED_DIR = os.path.join(BASE_DATA_PATH_INTERIM, "images_segmented_processed") # Assumes segmented images will be placed here
PROCESSED_IMAGES_CROPPED_DIR = os.path.join(BASE_DATA_PATH_INTERIM, "images_cropped_processed")   # Assumes cropped images will be placed here

# Create output directories if they don't exist
os.makedirs(BASE_DATA_PATH_PROCESSED, exist_ok=True)
os.makedirs(PROCESSED_IMAGES_ORIGINAL_DIR, exist_ok=True)
os.makedirs(PROCESSED_IMAGES_SEGMENTED_DIR, exist_ok=True)
os.makedirs(PROCESSED_IMAGES_CROPPED_DIR, exist_ok=True)

print(f"Raw Annotations: {RAW_ANNOTATIONS_FILE}")
print(f"Raw Images Dir: {RAW_IMAGES_DIR}")
print(f"Processed Original Images Dir: {PROCESSED_IMAGES_ORIGINAL_DIR}")
print(f"Processed Segmented Images Dir: {PROCESSED_IMAGES_SEGMENTED_DIR}")
print(f"Processed Cropped Images Dir: {PROCESSED_IMAGES_CROPPED_DIR}")
print(f"Processed Data Save Dir: {BASE_DATA_PATH_PROCESSED}")

# %% [markdown]
# ## 2. Load Raw Annotations

# %% [code]
def load_annotations(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        # Assuming the JSON is a list of QA items
        # If it's a dict like {"annotations": [...]}, adjust accordingly
        if isinstance(data, dict) and 'annotations' in data:
            df = pd.DataFrame(data['annotations'])
        elif isinstance(data, list):
            df = pd.DataFrame(data)
        else:
            raise ValueError("Annotations JSON structure not recognized. Expected list or dict with 'annotations' key.")
        print(f"Successfully loaded and parsed annotations from: {file_path}")
        print(f"Number of QA pairs loaded: {len(df)}")
        return df
    except FileNotFoundError:
        print(f"ERROR: Annotations file not found: {file_path}")
        return None
    except Exception as e:
        print(f"ERROR loading annotations: {e}")
        return None

raw_df = load_annotations(RAW_ANNOTATIONS_FILE)

if raw_df is not None:
    print("\nRaw DataFrame head:")
    print(raw_df.head())
    print("\nRaw DataFrame info:")
    raw_df.info()
    # EXPECTED COLUMNS (adjust if different):
    # - 'image_filename' (or 'image_id', 'file_name')
    # - 'question'
    # - 'answer'
    # - Optional: 'question_id', 'question_type' (for stratification or analysis)

    # Ensure essential columns exist
    required_cols = ['image_filename', 'question', 'answer'] # MODIFY IF YOUR COLUMN NAMES ARE DIFFERENT
    if not all(col in raw_df.columns for col in required_cols):
        print(f"ERROR: DataFrame is missing one or more required columns: {required_cols}. Found: {raw_df.columns.tolist()}")
        raw_df = None # Stop processing

# %% [markdown]
# ## 3. Text Preprocessing (Tokenization)
#
# We will use a BERT tokenizer to process questions and answers.

# %% [code]
if raw_df is not None:
    tokenizer = BertTokenizerFast.from_pretrained(BERT_MODEL_NAME)

    def tokenize_text(text_series, max_length):
        tokenized_output = tokenizer(
            text_series.tolist(),
            padding='max_length',    # Pad to max_length
            truncation=True,         # Truncate to max_length
            max_length=max_length,
            return_attention_mask=True,
            return_token_type_ids=True, # For BERT pair tasks, but good to have
            return_tensors='pt'      # Return PyTorch tensors (optional for saving, can convert later)
        )
        return tokenized_output

    # Tokenize questions
    print("\nTokenizing questions...")
    # Ensure questions are strings
    raw_df['question'] = raw_df['question'].astype(str)
    question_tokens = tokenize_text(raw_df['question'], MAX_TOKEN_LENGTH)
    raw_df['question_input_ids'] = question_tokens['input_ids'].tolist()
    raw_df['question_attention_mask'] = question_tokens['attention_mask'].tolist()
    raw_df['question_token_type_ids'] = question_tokens['token_type_ids'].tolist()
    print("Question tokenization complete.")

    # Tokenize answers
    # For VQA, answers are often shorter. You might use a different max_length or strategy.
    # Some VQA models treat answers as classification targets over a fixed vocabulary.
    # Here, we'll tokenize them similarly to questions, as the paper mentions BERT processing for answers too.
    print("\nTokenizing answers...")
    raw_df['answer'] = raw_df['answer'].astype(str) # Ensure answers are strings
    answer_tokens = tokenize_text(raw_df['answer'], MAX_TOKEN_LENGTH) # You might use a smaller max_length for answers
    raw_df['answer_input_ids'] = answer_tokens['input_ids'].tolist()
    raw_df['answer_attention_mask'] = answer_tokens['attention_mask'].tolist()
    raw_df['answer_token_type_ids'] = answer_tokens['token_type_ids'].tolist()
    print("Answer tokenization complete.")

    print("\nDataFrame head after tokenization:")
    print(raw_df[['question', 'question_input_ids', 'answer', 'answer_input_ids']].head())

# %% [markdown]
# ## 4. Image Preprocessing
#
# This involves:
# 1.  Processing **original** images (resize, normalize, save).
# 2.  Defining paths for **segmented** images. **Actual segmentation (e.g., with SAM) is assumed to be done by a separate script or pre-computed.** This notebook will include a placeholder.
# 3.  Defining paths for **cropped** images. **Actual cropping (e.g., based on bounding boxes from segmentation or detection) is assumed to be done by a separate script or pre-computed.** This notebook will include a placeholder.
#
# All processed image paths will be added to the DataFrame.

# %% [code]
if raw_df is not None:
    # Define image transformations
    # For models like ViT/BERT pre-trained on ImageNet, use ImageNet mean/std
    image_transform = transforms.Compose([
        transforms.Resize(IMAGE_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    def process_and_save_image(original_image_path, output_dir, image_filename, transform):
        """Loads, transforms, and saves an image. Returns the path to the saved image."""
        try:
            img = Image.open(original_image_path).convert("RGB")
            # Transformed image is a tensor. To save it as an image file,
            # we might need to convert it back or save the tensor directly (e.g. .pt file)
            # For simplicity in this example, we'll save a resized version without normalization
            # and assume normalization happens at load time during training.
            # OR: save the transformed tensor. Let's save a viewable resized image.
            
            resized_img = img.resize(IMAGE_SIZE)
            save_path = os.path.join(output_dir, image_filename)
            resized_img.save(save_path)
            return save_path
        except FileNotFoundError:
            print(f"Warning: Image not found at {original_image_path}, skipping.")
            return None
        except Exception as e:
            print(f"Warning: Could not process image {original_image_path}: {e}")
            return None

    # --- Placeholder functions for segmented and cropped images ---
    # IMPORTANT: Replace these with your actual logic or ensure these files are pre-generated.
    def get_or_generate_segmented_image_path(original_image_path, original_filename, output_dir):
        """
        Placeholder: Simulates getting/generating a segmented image.
        In reality, this would involve running SAM or loading a pre-segmented image.
        It copies the original and appends '_segmented'.
        """
        segmented_filename = f"{os.path.splitext(original_filename)[0]}_segmented{os.path.splitext(original_filename)[1]}"
        segmented_path = os.path.join(output_dir, segmented_filename)
        
        # --- SIMULATION: Replace this with actual segmentation ---
        if not os.path.exists(segmented_path) and os.path.exists(original_image_path):
             # For now, just copy the original if you want a file there.
             # Or better, ensure your real segmented files are in PROCESSED_IMAGES_SEGMENTED_DIR
             # For this placeholder, we will just return the expected path.
             # If you want to simulate file creation:
             # shutil.copy(original_image_path, segmented_path)
             pass # Assume it's pre-generated and path is correct
        # --- END SIMULATION ---
        
        # If your segmented images are pre-generated and have DIFFERENT names not derivable this way,
        # you'll need a mapping from original_filename to segmented_filename.
        return segmented_path # Return the expected path

    def get_or_generate_cropped_image_path(original_image_path, original_filename, output_dir):
        """
        Placeholder: Simulates getting/generating a cropped image.
        In reality, this would involve object detection/segmentation to get bounding boxes and then cropping.
        It copies the original and appends '_cropped'.
        """
        cropped_filename = f"{os.path.splitext(original_filename)[0]}_cropped{os.path.splitext(original_filename)[1]}"
        cropped_path = os.path.join(output_dir, cropped_filename)

        # --- SIMULATION: Replace this with actual cropping ---
        if not os.path.exists(cropped_path) and os.path.exists(original_image_path):
            # For this placeholder, we will just return the expected path.
            # If you want to simulate file creation:
            # shutil.copy(original_image_path, cropped_path)
            pass # Assume it's pre-generated and path is correct
        # --- END SIMULATION ---
        return cropped_path # Return the expected path

    # --- Apply to unique images to avoid redundant processing ---
    # Ensure 'image_filename' is the correct column name for your image files.
    image_filename_col = 'image_filename' # MODIFY IF YOURS IS 'image_id', 'file_name', etc.
    if image_filename_col not in raw_df.columns:
        raise ValueError(f"Column '{image_filename_col}' not found in DataFrame. Please specify the correct image filename column.")

    unique_image_filenames = raw_df[image_filename_col].unique()
    print(f"\nProcessing {len(unique_image_filenames)} unique images...")

    image_paths_map = {} # To store paths for each original filename

    for filename in tqdm(unique_image_filenames, desc="Processing Images"):
        original_img_path_raw = os.path.join(RAW_IMAGES_DIR, filename)
        
        # 1. Processed Original Image
        # Here we save a resized version. Normalization tensor is usually applied at training time.
        # If you want to save the normalized tensor directly, you'd use torch.save().
        # For this example, process_and_save_image saves a viewable resized image.
        processed_original_path = process_and_save_image(
            original_img_path_raw,
            PROCESSED_IMAGES_ORIGINAL_DIR,
            filename,
            image_transform # transform is not directly used by process_and_save_image for saving PIL, but good to pass
        )

        # 2. Segmented Image Path (Placeholder)
        # This assumes segmented images are already in PROCESSED_IMAGES_SEGMENTED_DIR
        # or your get_or_generate function handles it.
        # The filename for the segmented image might be different.
        # For this example, we derive it.
        base_name, ext = os.path.splitext(filename)
        segmented_filename = f"{base_name}_segmented{ext}" # Example naming convention
        processed_segmented_path_expected = os.path.join(PROCESSED_IMAGES_SEGMENTED_DIR, segmented_filename)
        # You would typically check if os.path.exists(processed_segmented_path_expected)

        # 3. Cropped Image Path (Placeholder)
        cropped_filename = f"{base_name}_cropped{ext}" # Example naming convention
        processed_cropped_path_expected = os.path.join(PROCESSED_IMAGES_CROPPED_DIR, cropped_filename)

        image_paths_map[filename] = {
            'original_processed_path': processed_original_path,
            'segmented_processed_path': processed_segmented_path_expected, # Path where it *should* be
            'cropped_processed_path': processed_cropped_path_expected    # Path where it *should* be
        }

    # Add these paths to the main DataFrame
    raw_df['original_processed_path'] = raw_df[image_filename_col].map(lambda fn: image_paths_map.get(fn, {}).get('original_processed_path'))
    raw_df['segmented_processed_path'] = raw_df[image_filename_col].map(lambda fn: image_paths_map.get(fn, {}).get('segmented_processed_path'))
    raw_df['cropped_processed_path'] = raw_df[image_filename_col].map(lambda fn: image_paths_map.get(fn, {}).get('cropped_processed_path'))

    # Drop rows where original image processing might have failed (e.g., file not found)
    # We use 'original_processed_path' as the primary check for successful image processing.
    initial_rows = len(raw_df)
    raw_df.dropna(subset=['original_processed_path'], inplace=True)
    if len(raw_df) < initial_rows:
        print(f"Warning: Dropped {initial_rows - len(raw_df)} rows due to missing/failed original image processing.")


    print("\nDataFrame head after adding image paths:")
    print(raw_df[[image_filename_col, 'original_processed_path', 'segmented_processed_path', 'cropped_processed_path']].head())
    print(f"\nCheck if segmented/cropped paths exist (example for first valid entry):")
    first_valid_paths_row = raw_df[raw_df['original_processed_path'].notna()].iloc[0]
    print(f"  Original : {first_valid_paths_row['original_processed_path']} (Exists: {os.path.exists(str(first_valid_paths_row['original_processed_path']))})")
    print(f"  Segmented: {first_valid_paths_row['segmented_processed_path']} (Exists: {os.path.exists(str(first_valid_paths_row['segmented_processed_path']))}) - REMINDER: This path is an expectation for pre-generated files.")
    print(f"  Cropped  : {first_valid_paths_row['cropped_processed_path']} (Exists: {os.path.exists(str(first_valid_paths_row['cropped_processed_path']))}) - REMINDER: This path is an expectation for pre-generated files.")


# %% [markdown]
# ## 5. Data Splitting (Train/Validation/Test)
#
# The paper mentions a 6:1:1 split (Training:Validation:Test).
# We will split based on unique image IDs/filenames to prevent data leakage between sets if multiple QAs belong to the same image.

# %% [code]
if raw_df is not None and not raw_df.empty:
    if image_filename_col not in raw_df.columns: # Check again, though it should exist if previous cell ran
        raise ValueError(f"Column '{image_filename_col}' not found in DataFrame for splitting.")

    unique_images = raw_df[image_filename_col].unique()
    
    # Calculate split sizes based on paper's 6:1:1 ratio (implies 8 parts total)
    # Train: 6/8 = 0.75
    # Val: 1/8 = 0.125
    # Test: 1/8 = 0.125
    
    # First, split into train and (validation + test)
    train_image_filenames, val_test_image_filenames = train_test_split(
        unique_images,
        test_size=(VALIDATION_SET_RATIO + TEST_SET_RATIO), # val_ratio + test_ratio
        random_state=RANDOM_STATE
    )
    
    # Then, split (validation + test) into validation and test
    # Adjust test_size for the second split: test_ratio / (val_ratio + test_ratio)
    relative_test_size = TEST_SET_RATIO / (VALIDATION_SET_RATIO + TEST_SET_RATIO)
    val_image_filenames, test_image_filenames = train_test_split(
        val_test_image_filenames,
        test_size=relative_test_size,
        random_state=RANDOM_STATE
    )
    
    print(f"Number of unique images: {len(unique_images)}")
    print(f"  Train images: {len(train_image_filenames)}")
    print(f"  Validation images: {len(val_image_filenames)}")
    print(f"  Test images: {len(test_image_filenames)}")

    # Create the data splits
    train_df = raw_df[raw_df[image_filename_col].isin(train_image_filenames)].copy()
    val_df = raw_df[raw_df[image_filename_col].isin(val_image_filenames)].copy()
    test_df = raw_df[raw_df[image_filename_col].isin(test_image_filenames)].copy()

    print(f"\nNumber of QA pairs:")
    print(f"  Train set: {len(train_df)}")
    print(f"  Validation set: {len(val_df)}")
    print(f"  Test set: {len(test_df)}")
    
    # Verify no overlap in image filenames between sets
    assert len(set(train_df[image_filename_col]) & set(val_df[image_filename_col])) == 0, "Overlap between train and val images!"
    assert len(set(train_df[image_filename_col]) & set(test_df[image_filename_col])) == 0, "Overlap between train and test images!"
    assert len(set(val_df[image_filename_col]) & set(test_df[image_filename_col])) == 0, "Overlap between val and test images!"
    print("\nNo image overlap confirmed between train, validation, and test sets.")

# %% [markdown]
# ## 6. Save Processed Data
#
# Save the processed DataFrames (train, validation, test) to JSON files or Parquet for efficiency.
# Each file will contain the QA pairs with tokenized text and paths to the three types of processed images.

# %% [code]
if 'train_df' in locals() and 'val_df' in locals() and 'test_df' in locals():
    try:
        # Select relevant columns to save. You might want to keep others like 'question_id' if they exist.
        columns_to_save = [
            image_filename_col, 'question', 'answer',
            'question_input_ids', 'question_attention_mask', 'question_token_type_ids',
            'answer_input_ids', 'answer_attention_mask', 'answer_token_type_ids',
            'original_processed_path', 'segmented_processed_path', 'cropped_processed_path'
        ]
        # Add any other columns you want to keep, e.g., 'question_type'
        if 'question_type' in train_df.columns: # Check if it exists from raw_df
            if 'question_type' not in columns_to_save: columns_to_save.append('question_type')


        # Filter out columns that might not exist if earlier steps failed for some reason
        train_df_save = train_df[[col for col in columns_to_save if col in train_df.columns]]
        val_df_save = val_df[[col for col in columns_to_save if col in val_df.columns]]
        test_df_save = test_df[[col for col in columns_to_save if col in test_df.columns]]

        # Save as JSON (easy to inspect) or Parquet (more efficient for large data)
        train_df_save.to_json(os.path.join(BASE_DATA_PATH_PROCESSED, "train_tlu_fruit_processed.json"), orient='records', indent=2)
        val_df_save.to_json(os.path.join(BASE_DATA_PATH_PROCESSED, "val_tlu_fruit_processed.json"), orient='records', indent=2)
        test_df_save.to_json(os.path.join(BASE_DATA_PATH_PROCESSED, "test_tlu_fruit_processed.json"), orient='records', indent=2)
        
        # Example for saving as Parquet (often preferred for DataLoaders)
        # train_df_save.to_parquet(os.path.join(BASE_DATA_PATH_PROCESSED, "train_tlu_fruit_processed.parquet"), index=False)
        # val_df_save.to_parquet(os.path.join(BASE_DATA_PATH_PROCESSED, "val_tlu_fruit_processed.parquet"), index=False)
        # test_df_save.to_parquet(os.path.join(BASE_DATA_PATH_PROCESSED, "test_tlu_fruit_processed.parquet"), index=False)

        print(f"\nProcessed data saved to: {BASE_DATA_PATH_PROCESSED}")
        print(f"  Train: train_tlu_fruit_processed.json ({len(train_df_save)} entries)")
        print(f"  Val:   val_tlu_fruit_processed.json ({len(val_df_save)} entries)")
        print(f"  Test:  test_tlu_fruit_processed.json ({len(test_df_save)} entries)")
    except Exception as e:
        print(f"ERROR saving processed data: {e}")

# %% [markdown]
# ## 7. Summary and Next Steps
#
# - Loaded raw TLU-Fruit annotations.
# - Tokenized questions and answers using BERT tokenizer.
# - Processed original images (resized and saved).
# - Established placeholder paths for segmented and cropped images (actual generation should be handled by dedicated scripts or pre-computed).
# - Split the data into train, validation, and test sets based on unique images.
# - Saved the processed datasets.
#
# **Next Steps:**
# - Implement actual generation of segmented and cropped images if not done already. Ensure the paths in the saved JSON/Parquet files point to these correctly.
# - `03_model_architecture.ipynb`: Define the VQA model architecture as described in the paper (CNN-ViT, BERT, Fusion module, PCA Transformer).
# - `04_train_model.ipynb`: Develop the training script, including data loaders for the processed data, loss functions, and the training loop.

# %% [code]
print("Preprocessing for TLU-Fruit dataset complete (with placeholders for segmentation/cropping).")