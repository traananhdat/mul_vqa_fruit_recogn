# %% [markdown]
# # 03. Model Prototyping
#
# This notebook defines the VQA model architecture based on the paper:
# "Transformer guided Multimodal VQA model for Fruit recognitions."
#
# We will define components for:
# 1. Multi-stream Image Encoding (Original, Segmented, Cropped via ViT).
# 2. Visual Feature Aggregation and Encoding.
# 3. PCA-based Feature Transformation (conceptual).
# 4. Text Encoding (BERT).
# 5. Multimodal Fusion (Fusion Transformer).
# 6. Multi-task Output Heads (Question Filtering and VQA Answer).

# %% [code]
import torch
import torch.nn as nn
import torch.nn.functional as F
from transformers import ViTModel, BertModel, ViTConfig, BertConfig
# For PCA, we'd typically use sklearn, but integrating it directly as a nn.Module for training
# requires careful handling (e.g., pre-fitting or custom layer).
# For this prototype, we'll define a conceptual PCA layer.
from sklearn.decomposition import PCA as SklearnPCA # For conceptual reference

# %% [markdown]
# ## 1. Configuration

# %% [code]
# Model Configurations (based on paper's Table 1 and common practices)
IMG_SIZE = 224 # Standard ViT input size
VIT_PATCH_SIZE = 16
VIT_HIDDEN_SIZE = 768 # ViT output dimension
VIT_NUM_LAYERS = 12 # ViT
VIT_NUM_HEADS = 12 # ViT

BERT_HIDDEN_SIZE = 768 # BERT output dimension (e.g., CLS token)
BERT_MODEL_NAME = 'bert-base-uncased' # Or another appropriate BERT model

# Visual Encoder (after concatenating 3 image streams and projecting)
VISUAL_ENCODER_DIM = 768
VISUAL_ENCODER_LAYERS = 4 # Paper mentions 12 for "Visual Encoder", but this can be heavy. Let's make it configurable.
VISUAL_ENCODER_HEADS = 8

# PCA Configuration
# Paper: "PCA: Dimensionality reduction on 768-dim image features"
# "Fusion Transformer" input suggests visual features are 768 after PCA to match 1536 total (768 vis + 768 text)
PCA_INPUT_DIM = VISUAL_ENCODER_DIM
PCA_OUTPUT_DIM = 768 # Transformed features, not necessarily reduced if it needs to be 768 for fusion.

# Fusion Module
FUSION_INPUT_DIM = PCA_OUTPUT_DIM + BERT_HIDDEN_SIZE # Should be 768 + 768 = 1536
FUSION_HIDDEN_DIM = 512
FUSION_DROPOUT = 0.5

# Output Heads
NUM_QUESTION_FILTER_CLASSES = 2 # "Question with fruit" / "Question w/o fruit"
NUM_VQA_ANSWER_CLASSES = 1000 # Placeholder: This should be the number of unique answers in your VQA task

# %% [markdown]
# ## 2. Model Components

# %% [markdown]
# ### 2.1. Image Stream Encoder (ViT-based)
#
# Each image stream (original, segmented, cropped) is processed by a ViT.

# %% [code]
class ImageStreamEncoder(nn.Module):
    def __init__(self, vit_model_name="google/vit-base-patch16-224-in21k", pretrained=True):
        super().__init__()
        # Load a pre-trained ViT model
        # The paper mentions ViT (12 Layers, Patch 16x16, 768 Hidden Units)
        # 'google/vit-base-patch16-224-in21k' matches this.
        if pretrained:
            self.vit = ViTModel.from_pretrained(vit_model_name)
        else:
            config = ViTConfig(hidden_size=VIT_HIDDEN_SIZE,
                               num_hidden_layers=VIT_NUM_LAYERS,
                               num_attention_heads=VIT_NUM_HEADS,
                               image_size=IMG_SIZE,
                               patch_size=VIT_PATCH_SIZE)
            self.vit = ViTModel(config)

    def forward(self, images):
        # images: (batch_size, num_channels, height, width)
        outputs = self.vit(pixel_values=images)
        # We use the CLS token's representation (pooler_output)
        # or the last hidden state of the CLS token
        return outputs.pooler_output # (batch_size, VIT_HIDDEN_SIZE)

# %% [markdown]
# ### 2.2. Visual Feature Aggregator & Encoder
#
# 1.  Takes features from the 3 image streams.
# 2.  Concatenates them.
# 3.  Projects them to `VISUAL_ENCODER_DIM`.
# 4.  Passes them through a Transformer Encoder (the "Visual Encoder" from Table 1).

# %% [code]
class VisualFeatureAggregator(nn.Module):
    def __init__(self, num_streams=3, stream_feature_dim=VIT_HIDDEN_SIZE,
                 encoder_dim=VISUAL_ENCODER_DIM, num_encoder_layers=VISUAL_ENCODER_LAYERS,
                 num_encoder_heads=VISUAL_ENCODER_HEADS, dropout=0.1):
        super().__init__()
        self.num_streams = num_streams
        self.concat_dim = num_streams * stream_feature_dim

        # Linear layer to project concatenated features to encoder_dim
        self.projection = nn.Linear(self.concat_dim, encoder_dim)

        # Transformer Encoder for the "Visual Encoder" part
        # (Paper Table 1: "Visual Encoder ... BertLayer (12 Layers, 768 Hidden, GELU)")
        # We'll use standard nn.TransformerEncoderLayer
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=encoder_dim,
            nhead=num_encoder_heads,
            dim_feedforward=encoder_dim * 4, # Common practice
            dropout=dropout,
            activation='gelu', # As per Table 1
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)
        self.layer_norm = nn.LayerNorm(encoder_dim)


    def forward(self, stream1_features, stream2_features, stream3_features):
        # streamN_features: (batch_size, VIT_HIDDEN_SIZE)
        concatenated_features = torch.cat([stream1_features, stream2_features, stream3_features], dim=1)
        # concatenated_features: (batch_size, 3 * VIT_HIDDEN_SIZE)

        projected_features = self.projection(concatenated_features)
        # projected_features: (batch_size, VISUAL_ENCODER_DIM)

        # TransformerEncoder expects input of shape (batch_size, seq_len, feature_dim)
        # Here, our "sequence" is just 1 element (the aggregated feature vector)
        # Or, it can operate on (seq_len, batch_size, feature_dim) if batch_first=False
        # Since we set batch_first=True, input should be (batch, seq_len=1, dim)
        encoder_input = projected_features.unsqueeze(1) # (batch_size, 1, VISUAL_ENCODER_DIM)
        
        encoded_output = self.transformer_encoder(encoder_input) # (batch_size, 1, VISUAL_ENCODER_DIM)
        encoded_output_squeezed = encoded_output.squeeze(1) # (batch_size, VISUAL_ENCODER_DIM)
        
        return self.layer_norm(encoded_output_squeezed)


# %% [markdown]
# ### 2.3. PCA Module (Conceptual / Wrapper)
#
# The paper calls this "PCA Transformer" but Table 1 describes it as "PCA (768-dim features)".
# This module will conceptually apply PCA. In a real setup, PCA would be fit offline on training features.
# For this prototype, if `do_pca_fit_on_batch` is True, it will fit and transform on the current batch (NOT recommended for actual training).
# Otherwise, it expects a pre-fitted PCA or acts as an identity/linear projection.

# %% [code]
class PCAModule(nn.Module):
    def __init__(self, input_dim=PCA_INPUT_DIM, output_dim=PCA_OUTPUT_DIM, fit_pca_on_batch=False, pre_fitted_pca_components=None):
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.fit_on_batch = fit_pca_on_batch # For PROTOTYPING ONLY
        self.pca_transformer = None

        if pre_fitted_pca_components is not None:
            # If pre-fitted PCA components (e.g., from scikit-learn) are provided
            # This is the more realistic scenario for inference/evaluation
            # For this, you'd save sklearn PCA's components_ and mean_
            # And implement the projection: (X - mean) @ components.T
            print(f"PCAModule: Using pre-fitted PCA components (conceptual, needs full implementation to use components). Output dim: {output_dim}")
            # For simplicity, if pre_fitted_pca_components is not None, we assume a linear projection
            # that mimics the dimensionality change. True PCA projection is more complex here.
            self.projection = nn.Linear(input_dim, output_dim) # Simplified stand-in
        elif not self.fit_on_batch:
             # If PCA is not fit on batch and no pre-fitted components,
             # it can be an identity if input_dim == output_dim, or a linear projection.
            print(f"PCAModule: Initialized without pre-fitted PCA and not fitting on batch. Using Linear for dim change if any. Output dim: {output_dim}")
            self.projection = nn.Linear(input_dim, output_dim)
        else: # fit_on_batch is True
            print(f"PCAModule: Initialized to fit PCA on each batch (PROTOTYPING/DEBUG ONLY). Output dim: {output_dim}")
            # No fixed projection layer here, will be created on the fly (bad practice for real models)
            pass

    def fit_transform_on_batch(self, x_numpy):
        # THIS IS FOR DEBUGGING/PROTOTYPING ONLY - DO NOT USE FOR ACTUAL TRAINING
        # PCA should be fit on the entire training set, not per batch.
        current_batch_size = x_numpy.shape[0]
        if current_batch_size < self.output_dim:
            # Cannot compute more components than samples
            print(f"Warning: Batch size {current_batch_size} is less than PCA output_dim {self.output_dim}. PCA might fail or produce fewer components.")
            n_components = min(current_batch_size, self.output_dim)
            if n_components == 0: return torch.zeros(current_batch_size, self.output_dim, device=x_numpy.device) # Or handle error
        else:
            n_components = self.output_dim
        
        if n_components > 0:
            pca = SklearnPCA(n_components=n_components)
            transformed_x_numpy = pca.fit_transform(x_numpy) # (batch_size, output_dim)
            # If n_components was reduced, pad with zeros to match self.output_dim
            if transformed_x_numpy.shape[1] < self.output_dim:
                padding = np.zeros((current_batch_size, self.output_dim - transformed_x_numpy.shape[1]))
                transformed_x_numpy = np.concatenate([transformed_x_numpy, padding], axis=1)
            return torch.tensor(transformed_x_numpy, dtype=torch.float32)
        else: # Should not happen if input dim > 0
            return torch.zeros(current_batch_size, self.output_dim, device=x_numpy.device)


    def forward(self, x):
        # x: (batch_size, input_dim)
        if self.fit_on_batch:
            # Ensure x is on CPU and is a numpy array for scikit-learn
            x_numpy = x.detach().cpu().numpy()
            transformed_x = self.fit_transform_on_batch(x_numpy)
            return transformed_x.to(x.device)
        elif hasattr(self, 'projection'): # Using pre-defined linear layer
            return self.projection(x)
        else: # Fallback if PCA components were meant to be used but projection logic is missing
            print("Warning: PCAModule using identity because no projection defined and not fitting on batch.")
            if self.input_dim != self.output_dim:
                # This case should ideally be handled by the self.projection in __init__
                raise ValueError("PCAModule input_dim != output_dim but no projection layer defined.")
            return x


# %% [markdown]
# ### 2.4. Text Encoder (BERT-based)

# %% [code]
class TextEncoder(nn.Module):
    def __init__(self, model_name=BERT_MODEL_NAME, pretrained=True):
        super().__init__()
        if pretrained:
            self.bert = BertModel.from_pretrained(model_name)
        else:
            config = BertConfig(hidden_size=BERT_HIDDEN_SIZE,
                                # ... other BERT params if needed
                               )
            self.bert = BertModel(config)
        # Freeze BERT parameters if needed for transfer learning
        # for param in self.bert.parameters():
        #     param.requires_grad = False

    def forward(self, input_ids, attention_mask, token_type_ids=None):
        # input_ids, attention_mask, token_type_ids: from BERT tokenizer
        outputs = self.bert(input_ids=input_ids,
                            attention_mask=attention_mask,
                            token_type_ids=token_type_ids)
        # Use the [CLS] token's representation (pooler_output)
        return outputs.pooler_output # (batch_size, BERT_HIDDEN_SIZE)

# %% [markdown]
# ### 2.5. Fusion Module ("Fusion Transformer")
#
# Table 1: LayerNorm, Linear(1536,512), ReLU, Dropout(0.5)
# Input: Concatenated (PCA-transformed Visual Features, BERT Text Features)

# %% [code]
class FusionModule(nn.Module):
    def __init__(self, input_dim=FUSION_INPUT_DIM, output_dim=FUSION_HIDDEN_DIM, dropout_rate=FUSION_DROPOUT):
        super().__init__()
        self.layer_norm = nn.LayerNorm(input_dim)
        self.linear = nn.Linear(input_dim, output_dim)
        self.relu = nn.ReLU()
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, visual_features, text_features):
        # visual_features: (batch_size, PCA_OUTPUT_DIM)
        # text_features: (batch_size, BERT_HIDDEN_SIZE)
        combined_features = torch.cat([visual_features, text_features], dim=1)
        # combined_features: (batch_size, FUSION_INPUT_DIM)

        x = self.layer_norm(combined_features)
        x = self.linear(x)
        x = self.relu(x)
        x = self.dropout(x)
        return x # (batch_size, FUSION_HIDDEN_DIM)

# %% [markdown]
# ### 2.6. Output Heads

# %% [code]
class QuestionFilterHead(nn.Module):
    def __init__(self, input_dim=FUSION_HIDDEN_DIM, num_classes=NUM_QUESTION_FILTER_CLASSES):
        super().__init__()
        self.linear = nn.Linear(input_dim, num_classes)

    def forward(self, fused_features):
        # fused_features: (batch_size, FUSION_HIDDEN_DIM)
        return self.linear(fused_features) # (batch_size, NUM_QUESTION_FILTER_CLASSES)

class VQAAnswerHead(nn.Module):
    def __init__(self, input_dim=FUSION_HIDDEN_DIM, num_classes=NUM_VQA_ANSWER_CLASSES):
        super().__init__()
        self.linear = nn.Linear(input_dim, num_classes)

    def forward(self, fused_features):
        # fused_features: (batch_size, FUSION_HIDDEN_DIM)
        return self.linear(fused_features) # (batch_size, NUM_VQA_ANSWER_CLASSES)

# %% [markdown]
# ## 3. Main VQA Model Assembly

# %% [code]
class FruitVQAModel(nn.Module):
    def __init__(self, config): # Pass a config dict or individual params
        super().__init__()
        self.config = config

        # Image Encoders (one for each stream)
        self.image_encoder_original = ImageStreamEncoder(vit_model_name=config.get('vit_model_name', "google/vit-base-patch16-224-in21k"))
        self.image_encoder_segmented = ImageStreamEncoder(vit_model_name=config.get('vit_model_name', "google/vit-base-patch16-224-in21k"))
        self.image_encoder_cropped = ImageStreamEncoder(vit_model_name=config.get('vit_model_name', "google/vit-base-patch16-224-in21k"))

        # Visual Feature Aggregator and Encoder
        self.visual_aggregator_encoder = VisualFeatureAggregator(
            num_streams=3,
            stream_feature_dim=config.get('vit_hidden_size', VIT_HIDDEN_SIZE),
            encoder_dim=config.get('visual_encoder_dim', VISUAL_ENCODER_DIM),
            num_encoder_layers=config.get('visual_encoder_layers', VISUAL_ENCODER_LAYERS),
            num_encoder_heads=config.get('visual_encoder_heads', VISUAL_ENCODER_HEADS)
        )

        # PCA Module
        self.pca_module = PCAModule(
            input_dim=config.get('visual_encoder_dim', VISUAL_ENCODER_DIM),
            output_dim=config.get('pca_output_dim', PCA_OUTPUT_DIM),
            fit_pca_on_batch=config.get('pca_fit_on_batch', False) # Should be False for real use
        )

        # Text Encoder
        self.text_encoder = TextEncoder(model_name=config.get('bert_model_name', BERT_MODEL_NAME))

        # Fusion Module
        self.fusion_module = FusionModule(
            input_dim=config.get('pca_output_dim', PCA_OUTPUT_DIM) + config.get('bert_hidden_size', BERT_HIDDEN_SIZE),
            output_dim=config.get('fusion_hidden_dim', FUSION_HIDDEN_DIM),
            dropout_rate=config.get('fusion_dropout', FUSION_DROPOUT)
        )

        # Output Heads
        self.question_filter_head = QuestionFilterHead(
            input_dim=config.get('fusion_hidden_dim', FUSION_HIDDEN_DIM),
            num_classes=config.get('num_question_filter_classes', NUM_QUESTION_FILTER_CLASSES)
        )
        self.vqa_answer_head = VQAAnswerHead(
            input_dim=config.get('fusion_hidden_dim', FUSION_HIDDEN_DIM),
            num_classes=config.get('num_vqa_answer_classes', NUM_VQA_ANSWER_CLASSES)
        )

    def forward(self, image_original, image_segmented, image_cropped,
                question_input_ids, question_attention_mask, question_token_type_ids=None):

        # 1. Image Features
        feat_original = self.image_encoder_original(image_original)
        feat_segmented = self.image_encoder_segmented(image_segmented)
        feat_cropped = self.image_encoder_cropped(image_cropped)

        # 2. Aggregate and Encode Visual Features
        aggregated_visual_features = self.visual_aggregator_encoder(feat_original, feat_segmented, feat_cropped)

        # 3. PCA Transformation
        pca_visual_features = self.pca_module(aggregated_visual_features)

        # 4. Text Features
        text_features = self.text_encoder(question_input_ids, question_attention_mask, question_token_type_ids)

        # 5. Fusion
        fused_representation = self.fusion_module(pca_visual_features, text_features)

        # 6. Output Heads
        filter_logits = self.question_filter_head(fused_representation)
        vqa_answer_logits = self.vqa_answer_head(fused_representation)

        return filter_logits, vqa_answer_logits

# %% [markdown]
# ## 4. Instantiate Model and Test Forward Pass

# %% [code]
# Create a configuration dictionary
model_config = {
    'vit_model_name': "google/vit-base-patch16-224-in21k", # Ensure this model exists or is downloadable
    'vit_hidden_size': VIT_HIDDEN_SIZE,
    'visual_encoder_dim': VISUAL_ENCODER_DIM,
    'visual_encoder_layers': VISUAL_ENCODER_LAYERS,
    'visual_encoder_heads': VISUAL_ENCODER_HEADS,
    'pca_output_dim': PCA_OUTPUT_DIM,
    'pca_fit_on_batch': True, # SET TO TRUE ONLY FOR THIS DUMMY TEST to make PCA run.
    'bert_model_name': BERT_MODEL_NAME,
    'bert_hidden_size': BERT_HIDDEN_SIZE,
    'fusion_hidden_dim': FUSION_HIDDEN_DIM,
    'fusion_dropout': FUSION_DROPOUT,
    'num_question_filter_classes': NUM_QUESTION_FILTER_CLASSES,
    'num_vqa_answer_classes': NUM_VQA_ANSWER_CLASSES
}

# Instantiate the model
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Wrap model instantiation in try-except for Hugging Face download issues if offline
try:
    vqa_model = FruitVQAModel(model_config).to(device)
    print("Model instantiated successfully.")
except Exception as e:
    print(f"Error instantiating model (possibly due to Hugging Face model download): {e}")
    vqa_model = None


if vqa_model:
    # Create dummy inputs
    batch_size = 2 # Small batch for testing
    dummy_image_original = torch.randn(batch_size, 3, IMG_SIZE, IMG_SIZE).to(device)
    dummy_image_segmented = torch.randn(batch_size, 3, IMG_SIZE, IMG_SIZE).to(device) # Assuming segmented is also RGB
    dummy_image_cropped = torch.randn(batch_size, 3, IMG_SIZE, IMG_SIZE).to(device)

    # Dummy text inputs (tokenized)
    # MAX_TOKEN_LENGTH is defined in config section
    dummy_question_input_ids = torch.randint(0, 1000, (batch_size, MAX_TOKEN_LENGTH)).to(device) # Vocab size 1000 placeholder
    dummy_question_attention_mask = torch.ones(batch_size, MAX_TOKEN_LENGTH, dtype=torch.long).to(device)
    dummy_question_token_type_ids = torch.zeros(batch_size, MAX_TOKEN_LENGTH, dtype=torch.long).to(device)

    # Test forward pass
    try:
        print("\nTesting forward pass...")
        vqa_model.train() # Set to train mode (matters for dropout, and our dummy PCA)
        filter_logits, vqa_logits = vqa_model(dummy_image_original,
                                              dummy_image_segmented,
                                              dummy_image_cropped,
                                              dummy_question_input_ids,
                                              dummy_question_attention_mask,
                                              dummy_question_token_type_ids)
        print("Forward pass successful!")
        print("Filter Logits Shape:", filter_logits.shape) # Expected: (batch_size, NUM_QUESTION_FILTER_CLASSES)
        print("VQA Answer Logits Shape:", vqa_logits.shape)   # Expected: (batch_size, NUM_VQA_ANSWER_CLASSES)

        # Test eval mode (PCA should ideally use fitted components here)
        # For our dummy PCA with fit_on_batch=True, this won't change PCA behavior much
        # but good practice for other layers like Dropout.
        vqa_model.eval()
        with torch.no_grad():
             filter_logits_eval, vqa_logits_eval = vqa_model(dummy_image_original,
                                                  dummy_image_segmented,
                                                  dummy_image_cropped,
                                                  dummy_question_input_ids,
                                                  dummy_question_attention_mask,
                                                  dummy_question_token_type_ids)
        print("\nForward pass in eval mode successful!")
        print("Filter Logits Shape (eval):", filter_logits_eval.shape)
        print("VQA Answer Logits Shape (eval):", vqa_logits_eval.shape)


    except Exception as e:
        print(f"Error during forward pass: {e}")
        import traceback
        traceback.print_exc()

# %% [markdown]
# ## 5. Model Summary (Optional)
#
# If you have `torchsummary` installed, you can print a summary.
# Note: `torchsummary` might have issues with models having multiple complex inputs directly.

# %% [code]
if vqa_model:
    num_params = sum(p.numel() for p in vqa_model.parameters() if p.requires_grad)
    print(f"\nTotal trainable parameters in VQAModel: {num_params:,}")

    # from torchsummary import summary
    # This model has multiple inputs, summary(model, input_size_tuple_of_tuples) is complex.
    # For example:
    # summary(vqa_model.image_encoder_original.vit, (3, IMG_SIZE, IMG_SIZE))
    # summary(vqa_model.text_encoder.bert, input_size=[(MAX_TOKEN_LENGTH,), (MAX_TOKEN_LENGTH,),(MAX_TOKEN_LENGTH,)] # for ids, mask, type_ids - careful with dtypes
    # Printing specific sub-modules is easier.
    print("\nSample Sub-module (Image Encoder Original - ViT):")
    print(vqa_model.image_encoder_original.vit)


# %% [markdown]
# ## Next Steps:
# - **PCA Fitting:** For a real implementation, the `PCAModule` needs to be properly handled. You would fit `SklearnPCA` on the features extracted from your entire training dataset (output of `VisualFeatureAggregator`) *once*. Then, save the `pca.components_` and `pca.mean_`. The `PCAModule`'s forward pass would then implement the transformation: `(X - mean) @ components.T` using these pre-fitted values, converted to PyTorch tensors and layers (e.g., a non-trainable Linear layer or direct matrix multiplication).
# - **Dataset and DataLoader:** Create PyTorch Datasets and DataLoaders for your preprocessed data from `02_preprocessing_TLU-Fruit.ipynb`.
# - **Training Loop:** Develop the training script (`04_train_model.ipynb`) incorporating the loss functions (CrossEntropy for classification heads) and optimizer. The paper mentions a multi-task loss (Section 3.3).
# - **Answer Vocabulary:** For the `VQAAnswerHead`, you need to define `NUM_VQA_ANSWER_CLASSES` based on your dataset (e.g., top-K answers). The model will output logits, and you'll typically map these to actual answer strings during evaluation.