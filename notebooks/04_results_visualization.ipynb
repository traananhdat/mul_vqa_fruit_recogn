# %% [markdown]
# # 04. Results Visualization
#
# This notebook is dedicated to visualizing the results obtained from the VQA model training and evaluation, as presented in the paper "Transformer guided Multimodal VQA model for Fruit recognitions."
#
# We will cover:
# 1. Visualization of Performance Metrics (Tables & Bar Charts).
# 2. Qualitative Analysis: Attention Map Visualization (conceptual, like Figure 2).
# 3. Deep Learning Visualizations:
#     - t-SNE plots of learned features.
#     - Distribution plots of predicted scores.
# 4. (Optional) Examples of Model Predictions (Correct and Incorrect).

# %% [code]
import json
import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from PIL import Image
from sklearn.manifold import TSNE
# from sklearn.metrics import confusion_matrix # Optional for error analysis

# Matplotlib and Seaborn settings
sns.set_theme(style="whitegrid")
plt.rcParams['figure.figsize'] = (12, 7)
plt.rcParams['figure.dpi'] = 100

# %% [markdown]
# ## 1. Load Results Data (Placeholders)
#
# Load your saved metrics, predictions, attention weights, and features.
# You might load from CSV, JSON, Pickle files, or define data directly for tables.

# %% [code]
# --- Placeholder Paths ---
# Replace these with your actual file paths or data loading methods
RESULTS_DIR = "../reports/model_results/" # Example directory
TABLE2_DATA_PATH = os.path.join(RESULTS_DIR, "table2_sota_comparison.csv")
TABLE4_DATA_PATH = os.path.join(RESULTS_DIR, "table4_ablation_study.csv")
TABLE5_DATA_PATH = os.path.join(RESULTS_DIR, "table5_hyperparam_sensitivity.csv")

# For qualitative examples, attention, features, predictions
# These would typically be saved during your evaluation script
SAMPLE_PREDICTIONS_PATH = os.path.join(RESULTS_DIR, "sample_predictions_with_attention.pkl") # Example, could be .pt, .npz
EXTRACTED_FEATURES_PATH = os.path.join(RESULTS_DIR, "extracted_features_for_tsne.pkl")
PREDICTED_SCORES_PATH = os.path.join(RESULTS_DIR, "predicted_scores_distribution.pkl")

# --- Load Data (Example using direct definition for tables, replace with file loading if preferred) ---

# Data for Table 2: Comparison with SOTA (COCO-QA and TLU-Fruit)
# Manually enter or load from CSV. Ensure 'Model' is the index or a column.
# WUPS@0.9 is written as 'W@0.9' in the paper's table.
table2_data = {
    'Model': ['DPPnet [17]', 'VinVL [15]', 'Dual-MFA [18]', 'MCAN [16]', 'ODA [19]', 'v-AGCN [20]', 'ALSA [21]', 'MCAN+PA [22]', 'Rank VQA [14]', 'Ours model'],
    'COCO_Accuracy': [0.6119, 0.705, 0.6649, 0.6808, 0.6933, 0.7013, 0.6997, 0.7010, 0.7268, 0.7364],
    'COCO_F1': [0.6042, 0.6990, 0.6426, 0.6625, 0.6734, 0.6928, 0.6842, 0.6892, 0.7153, 0.7042],
    'COCO_WUPS09': [0.7084, 0.7942, 0.7615, 0.7734, 0.7829, 0.8043, 0.7943, 0.7943, 0.8104, 0.8295],
    'TLU_Accuracy': [0.5589, 0.6398, 0.5984, 0.6135, 0.6212, np.nan, np.nan, 0.6232, 0.6397, 0.6615], # v-AGCN, ALSA missing TLU
    'TLU_F1': [0.5537, 0.6312, 0.5923, 0.6175, 0.6278, np.nan, np.nan, 0.6165, 0.6313, 0.6604],
    'TLU_WUPS09': [0.5693, 0.6475, 0.6043, 0.6206, 0.6356, np.nan, np.nan, 0.6298, 0.6454, 0.6695],
}
try:
    # Try loading from file first if it exists
    df_table2 = pd.read_csv(TABLE2_DATA_PATH)
    print(f"Loaded Table 2 data from {TABLE2_DATA_PATH}")
except FileNotFoundError:
    print(f"{TABLE2_DATA_PATH} not found, using manually defined data for Table 2.")
    df_table2 = pd.DataFrame(table2_data)

# Data for Table 4: Ablation Study (TLU-Fruit)
# Exp. | Image Backbones   | Text Backbones | Accuracy | F1-Score | W@0.9
table4_data = {
    'Experiment': [1, 2, 3, 4, 5],
    'Config_Details': [
        "CNN (Img) | None (Txt)",
        "CNN-ViT (Img) | None (Txt)",
        "CNN-ViT (Img) | BERT (Txt)",
        "PCA-CNN-ViT (Img) | BERT (Txt)",
        "PCA-CNN-ViT (Img) | PCA-BERT (Txt)" # Paper refers to PCA-BERT-ViT
    ],
    'Accuracy': [0.5422, 0.5734, 0.6482, 0.6484, 0.6615],
    'F1-Score': [0.5486, 0.5692, 0.6423, 0.6474, 0.6604],
    'WUPS09': [0.5573, 0.5843, 0.6583, 0.6529, 0.6695]
}
try:
    df_table4 = pd.read_csv(TABLE4_DATA_PATH)
    print(f"Loaded Table 4 data from {TABLE4_DATA_PATH}")
except FileNotFoundError:
    print(f"{TABLE4_DATA_PATH} not found, using manually defined data for Table 4.")
    df_table4 = pd.DataFrame(table4_data)


# Data for Table 5: Hyperparameter Sensitivity (Loss function weights)
# lambda1 | lambda2 | lambda3 | Accuracy | F1-Score | W@0.9
table5_data = {
    'lambda1': [0.1, 0.1, 0.1, 0.5, 0.5, 0.5, 0.9, 0.9, 0.9], # lambda2 col in paper is actually lambda1 values for rows with lambda2=0.9, 0.5, 0.1
    'lambda2': [0.1, 0.5, 0.9, 0.1, 0.5, 0.9, 0.1, 0.5, 1.0], # Correcting based on reading table structure
    'lambda3': [0.8, 0.4, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0], # lambda3 values, need to align with combinations. This interpretation may need checking against actual table structure if paper is ambiguous.
                                                              # The table in paper has missing lambda3 values, assuming 0 or aligned by row.
                                                              # Let's use the paper's direct values if possible, mapping rows
    # Re-interpreting Table 5 from the paper structure
    # Col1=lambda1, Col2=lambda2 (paper calls it "13" but is lambda2), Col3=lambda3
    table5_paper_data = {
        'lambda1_setting': ['0.1','0.1','0.1', '0.5','0.5','0.5', '0.9','0.9','0.9'], # Row group
        'lambda1':         [0.1,  0.1,  0.1,   0.5,  0.5,  0.5,   0.9,  0.9,  0.9],
        'lambda2':         [0.1,  0.5,  0.9,   0.1,  0.5,  0.1,   0.1,  0.5,  1.0], # Note: paper has 0.9 as a col val for lambda3, this might be lambda2
        'lambda3':         [0.8,  0.4,  None,  0.4,  None, 0.5,   None, 0.5,  None], # Paper table is sparse
        'Accuracy':        [0.58, 0.60, 0.62,  0.66, 0.63, 0.61,  0.63, 0.57, 0.54],
        'F1-Score':        [0.58, 0.61, 0.62,  0.66, 0.63, 0.62,  0.64, 0.56, 0.54],
        'WUPS09':          [0.59, 0.61, 0.63,  0.67, 0.64, 0.62,  0.64, 0.58, 0.55]
    }
    # The optimal from paper: lambda1=0.5, lambda2=0.1, lambda3=0.4 => Acc 0.66
    # The table presentation in the PDF is a bit confusing, will use a simplified version for plotting.
    # For plotting, we might focus on varying one lambda while keeping others constant or show the best one.
    # For this example, let's use the explicit values mentioned for the best result and a few others.
    df_table5_simplified = pd.DataFrame({
        'Config': ['L1=0.1,L2=0.1,L3=0.8', 'L1=0.1,L2=0.5,L3=0.4', 'L1=0.5,L2=0.1,L3=0.4 (Optimal)', 'L1=0.9,L2=0.1,L3=0.0 (example)'],
        'Accuracy': [0.58, 0.60, 0.66, 0.63], # Example values
        'F1-Score': [0.58, 0.61, 0.66, 0.64],
        'WUPS09':   [0.59, 0.61, 0.67, 0.64]
    })
else:
    df_table5_simplified = None # Initialize if no data available

print("--- Table 2 Data (SOTA Comparison) ---")
print(df_table2.to_string())
print("\n--- Table 4 Data (Ablation Study) ---")
print(df_table4.to_string())
if df_table5_simplified is not None:
    print("\n--- Table 5 Data (Hyperparameter Sensitivity - Simplified for Plot) ---")
    print(df_table5_simplified.to_string())

# %% [markdown]
# ## 2. Visualize Performance Metrics

# %% [markdown]
# ### 2.1. SOTA Comparison (from Table 2)

# %% [code]
if df_table2 is not None:
    # Plot for COCO-QA
    df_table2_coco = df_table2[['Model', 'COCO_Accuracy', 'COCO_F1', 'COCO_WUPS09']].copy()
    df_table2_coco.set_index('Model', inplace=True)
    df_table2_coco.rename(columns={'COCO_Accuracy': 'Accuracy', 'COCO_F1': 'F1-Score', 'COCO_WUPS09': 'WUPS@0.9'}, inplace=True)
    
    df_table2_coco.plot(kind='bar', figsize=(14, 8), rot=45)
    plt.title('Model Performance on COCO-QA Dataset')
    plt.ylabel('Score')
    plt.xlabel('Model')
    plt.legend(title='Metrics')
    plt.tight_layout()
    plt.show()

    # Plot for TLU-Fruit (filter out models with NaN for TLU-Fruit)
    df_table2_tlu = df_table2[['Model', 'TLU_Accuracy', 'TLU_F1', 'TLU_WUPS09']].copy()
    df_table2_tlu.dropna(subset=['TLU_Accuracy'], inplace=True) # Remove rows where TLU_Accuracy is NaN
    df_table2_tlu.set_index('Model', inplace=True)
    df_table2_tlu.rename(columns={'TLU_Accuracy': 'Accuracy', 'TLU_F1': 'F1-Score', 'TLU_WUPS09': 'WUPS@0.9'}, inplace=True)

    if not df_table2_tlu.empty:
        df_table2_tlu.plot(kind='bar', figsize=(14, 8), rot=45)
        plt.title('Model Performance on TLU-Fruit Dataset')
        plt.ylabel('Score')
        plt.xlabel('Model')
        plt.legend(title='Metrics')
        plt.tight_layout()
        plt.show()
    else:
        print("No data to plot for TLU-Fruit after dropping NaNs.")

# %% [markdown]
# ### 2.2. Ablation Study (from Table 4)

# %% [code]
if df_table4 is not None:
    df_table4_plot = df_table4.set_index('Config_Details') # Use experiment number or config details as index
    
    df_table4_plot.plot(kind='bar', figsize=(14, 8), rot=30)
    plt.title('Ablation Study on TLU-Fruit Dataset (Table 4)')
    plt.ylabel('Score')
    plt.xlabel('Experiment Configuration')
    plt.legend(title='Metrics')
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ### 2.3. Hyperparameter Sensitivity (from Table 5 - Simplified)

# %% [code]
if df_table5_simplified is not None:
    df_table5_plot = df_table5_simplified.set_index('Config')
    
    df_table5_plot.plot(kind='bar', figsize=(12, 7), rot=30)
    plt.title('Hyperparameter Sensitivity (Loss Weights - Table 5 Simplified)')
    plt.ylabel('Score')
    plt.xlabel('Lambda Configuration (L1, L2, L3)')
    plt.legend(title='Metrics')
    plt.tight_layout()
    plt.show()

# %% [markdown]
# ## 3. Qualitative Analysis - Attention Visualization
#
# This section requires saved attention weights from your model for specific (image, question) pairs.
# The visualization will be a template; you'll need to adapt it based on how your model's attention is structured and saved.
# Figure 2 in the paper shows attention between question words, answer words (or predicted answer tokens), and task relevance.
# Here, we'll aim for a general image-text attention overlay if patch-based attention is available.

# %% [code]
# --- Placeholder: Load sample data with attention weights ---
# This data would ideally be a dictionary or list of dictionaries containing:
# 'image_path': path to the image file
# 'question': string
# 'attention_weights': numpy array or torch tensor of attention scores
#                      Shape might be (num_text_tokens, num_image_patches) or similar
#                      Or (num_heads, seq_len_q, seq_len_kv) for transformer attention
# 'image_patches': if attention is over patches, you might need patch coordinates or the patched image
# 'text_tokens': list of text tokens corresponding to the attention dimension

# Example:
# try:
#     with open(SAMPLE_PREDICTIONS_PATH, 'rb') as f:
#         sample_data_for_attention = pickle.load(f) # Assuming pickle format
#     print(f"Loaded sample data for attention from {SAMPLE_PREDICTIONS_PATH}")
# except FileNotFoundError:
#     print(f"Attention data file {SAMPLE_PREDICTIONS_PATH} not found. Skipping attention visualization.")
sample_data_for_attention = [] # Initialize to empty

# For demonstration, let's create a dummy sample if no file is loaded
if not sample_data_for_attention:
    print("Creating dummy data for attention visualization demo.")
    # Create a dummy image file
    dummy_image_path = "dummy_attention_image.png"
    try:
        Image.new('RGB', (224, 224), color = 'skyblue').save(dummy_image_path)
        dummy_attention_weights = np.random.rand(10, 14*14) # 10 text tokens, 14x14=196 image patches (for 224x224 img, 16x16 patch)
        dummy_text_tokens = ["what", "color", "is", "the", "sky", "?", "[SEP]", "[PAD]", "[PAD]", "[PAD]"]
        sample_data_for_attention = [{
            'image_path': dummy_image_path,
            'question': "what color is the sky ?",
            'attention_weights': dummy_attention_weights, # (num_text_tokens, num_image_patches)
            'text_tokens': dummy_text_tokens,
            'patch_size': 16 # For reshaping attention to image grid
        }]
    except Exception as e:
        print(f"Error creating dummy image for attention demo: {e}")
        sample_data_for_attention = []


def visualize_text_to_image_attention(image_path, text_tokens, attention_weights, patch_size, token_idx_to_viz, threshold=0.1):
    """
    Visualizes attention from a specific text token to image patches.
    Args:
        image_path (str): Path to the original image.
        text_tokens (list): List of text tokens.
        attention_weights (np.array): Attention array, e.g., (num_text_tokens, num_image_patches).
        patch_size (int): Size of each image patch (e.g., 16 for ViT).
        token_idx_to_viz (int): Index of the text token for which to visualize attention.
        threshold (float): Minimum attention weight to display.
    """
    try:
        img = Image.open(image_path).convert("RGB")
    except FileNotFoundError:
        print(f"Image not found: {image_path}")
        return

    if token_idx_to_viz >= len(text_tokens):
        print(f"token_idx_to_viz {token_idx_to_viz} is out of bounds for text_tokens (len {len(text_tokens)}).")
        return
        
    token_attention = attention_weights[token_idx_to_viz, :] # Attention from one token to all patches

    # Reshape attention to image grid
    # Assuming square image and square patches
    img_w, img_h = img.size
    num_patches_h = img_h // patch_size
    num_patches_w = img_w // patch_size
    
    if token_attention.shape[0] != num_patches_h * num_patches_w:
        print(f"Attention dimension ({token_attention.shape[0]}) doesn't match expected patches ({num_patches_h * num_patches_w}). Skipping.")
        # This can happen if ViT adds a CLS token patch, or if image size/patch size assumptions are wrong.
        # For ViT, often the first "patch" is the CLS token, so attention might be over (N_patches + 1).
        # Let's assume attention_weights are for actual image patches here.
        # If your attention_weights include CLS token, you might need to skip it: token_attention = attention_weights[token_idx_to_viz, 1:]

        # Try to adjust if CLS token is involved and weights are (num_text_tokens, num_image_patches + 1)
        if token_attention.shape[0] == num_patches_h * num_patches_w + 1:
            print("Adjusting for CLS token in attention weights by skipping the first attention value.")
            token_attention = token_attention[1:] 
        else:
            print(f"Cannot reconcile attention dim {token_attention.shape[0]} with patch grid {num_patches_h * num_patches_w}")
            return

    try:
        attention_map = token_attention.reshape(num_patches_h, num_patches_w)
    except ValueError as e:
        print(f"Error reshaping attention map: {e}. Shape was {token_attention.shape}, target {num_patches_h}x{num_patches_w}")
        return

    # Upscale attention map to image size for overlay
    attention_map_resized = Image.fromarray(attention_map).resize(img.size, resample=Image.BILINEAR)
    
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.imshow(img)
    plt.title(f"Original Image\nQ: {' '.join(text_tokens)}")
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(img)
    plt.imshow(np.array(attention_map_resized), alpha=0.6, cmap='viridis') # Overlay attention
    plt.title(f"Attention for token: '{text_tokens[token_idx_to_viz]}'")
    plt.axis('off')
    plt.tight_layout()
    plt.show()

if sample_data_for_attention:
    print("\n--- Visualizing Attention (Example) ---")
    for sample in sample_data_for_attention:
        # Visualize attention for a few tokens, e.g., the first content word
        token_to_visualize = 0
        while token_to_visualize < len(sample['text_tokens']) and sample['text_tokens'][token_to_visualize] in ['[CLS]', '[SEP]', '[PAD]']:
            token_to_visualize += 1
        if token_to_visualize < len(sample['text_tokens']):
             visualize_text_to_image_attention(
                image_path=sample['image_path'],
                text_tokens=sample['text_tokens'],
                attention_weights=sample['attention_weights'],
                patch_size=sample.get('patch_size', 16), # Default to 16 if not specified
                token_idx_to_viz=token_to_visualize
            )
        else:
            print(f"No content token found to visualize for question: {sample['question']}")
else:
    print("No sample data with attention weights loaded/defined. Skipping attention visualization.")

if os.path.exists("dummy_attention_image.png"):
    os.remove("dummy_attention_image.png") # Clean up dummy file

# %% [markdown]
# ## 4. Deep Learning Visualizations

# %% [markdown]
# ### 4.1. t-SNE Plot of Learned Features
#
# This requires saved feature vectors (e.g., from the fusion layer) and corresponding labels (e.g., true answer categories or question types).

# %% [code]
# --- Placeholder: Load extracted features and labels ---
# features_array: numpy array of shape (num_samples, feature_dimension)
# labels_array: numpy array of shape (num_samples,) representing categories

# try:
#     with open(EXTRACTED_FEATURES_PATH, 'rb') as f:
#         data_for_tsne = pickle.load(f) # Expects a dict {'features': np.array, 'labels': np.array}
#     features_array = data_for_tsne['features']
#     labels_array = data_for_tsne['labels']
#     # If labels are answer strings, you might want to map them to integer IDs first
#     # Or use question_type if that's what you want to visualize separation by.
#     unique_labels = np.unique(labels_array)
#     if len(unique_labels) > 20: # Limit number of classes for better t-SNE viz
#         print(f"Warning: Too many unique labels ({len(unique_labels)}) for t-SNE plot. Consider filtering or grouping.")
#     print(f"Loaded features ({features_array.shape}) and labels ({labels_array.shape}) for t-SNE.")
# except FileNotFoundError:
#     print(f"Feature file {EXTRACTED_FEATURES_PATH} not found. Creating dummy data for t-SNE.")
features_array = np.random.rand(150, 50) # 150 samples, 50 features (e.g., output of fusion layer)
labels_array = np.random.randint(0, 5, 150) # 5 dummy classes
# except Exception as e:
#     print(f"Error loading t-SNE data: {e}. Creating dummy data.")
#     features_array = np.random.rand(150, 50)
#     labels_array = np.random.randint(0, 3, 150)


if features_array is not None and labels_array is not None:
    print("\n--- Running t-SNE (this may take a moment)... ---")
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(features_array)-1), n_iter=300) # Adjust perplexity
    tsne_results = tsne.fit_transform(features_array)

    plt.figure(figsize=(10, 8))
    sns.scatterplot(
        x=tsne_results[:,0], y=tsne_results[:,1],
        hue=labels_array,
        palette=sns.color_palette("hsv", len(np.unique(labels_array))), # Color by label
        legend="full",
        alpha=0.7
    )
    plt.title('t-SNE Visualization of Learned Features')
    plt.xlabel('t-SNE Component 1')
    plt.ylabel('t-SNE Component 2')
    plt.legend(title='Category/Label')
    plt.show()
else:
    print("No feature data loaded for t-SNE visualization.")


# %% [markdown]
# ### 4.2. Distribution of Predicted Scores
#
# This requires saved predicted probabilities or scores from your model's output heads.

# %% [code]
# --- Placeholder: Load predicted scores ---
# predicted_scores: numpy array of probabilities (e.g., from softmax layer of VQA head or filter head)
# ground_truth_labels: corresponding true labels for these scores (optional, for conditional plotting)
# is_correct: boolean array indicating if the prediction was correct (optional)

# try:
#     with open(PREDICTED_SCORES_PATH, 'rb') as f:
#         data_for_scores_dist = pickle.load(f) # Expects {'scores': np.array, 'is_correct': np.array (optional)}
#     predicted_scores = data_for_scores_dist['scores'] # e.g., confidence in the predicted answer
#     is_correct = data_for_scores_dist.get('is_correct') # Optional
#     print(f"Loaded predicted scores ({predicted_scores.shape}) for distribution plot.")
# except FileNotFoundError:
#     print(f"Predicted scores file {PREDICTED_SCORES_PATH} not found. Creating dummy data.")
predicted_scores = np.random.rand(500) # 500 dummy prediction confidences
is_correct = np.random.choice([True, False], 500) if np.random.rand() > 0.1 else None # Occasionally don't create it
# except Exception as e:
#     print(f"Error loading predicted scores data: {e}. Creating dummy data.")
#     predicted_scores = np.random.rand(500)
#     is_correct = None

if predicted_scores is not None:
    plt.figure(figsize=(10, 6))
    if is_correct is not None and len(predicted_scores) == len(is_correct):
        sns.histplot(predicted_scores[is_correct], color="skyblue", label='Correct Predictions', kde=True, stat="density", common_norm=False)
        sns.histplot(predicted_scores[~is_correct], color="orange", label='Incorrect Predictions', kde=True, stat="density", common_norm=False)
        plt.legend()
    else:
        sns.histplot(predicted_scores, color="skyblue", kde=True)
    
    plt.title('Distribution of Predicted Scores/Confidences')
    plt.xlabel('Predicted Score/Confidence')
    plt.ylabel('Density')
    plt.show()
else:
    print("No predicted score data loaded for distribution plot.")

# %% [markdown]
# ## 5. (Optional) Display Sample Predictions
#
# Show some images with their questions, ground truth answers, and the model's predicted answers.
# This helps in understanding specific failure or success cases.

# %% [code]
# --- Placeholder: Load sample predictions data ---
# This would typically be a list of dicts, each containing:
# 'image_path', 'question', 'ground_truth_answer', 'predicted_answer', 'is_correct' (boolean)

# try:
#     with open(SAMPLE_PREDICTIONS_PATH, 'rb') as f: # May reuse the attention data if it contains predictions
#         sample_predictions = pickle.load(f)
#     if isinstance(sample_predictions, list) and 'predicted_answer' not in sample_predictions[0]:
#         # If loaded attention data doesn't have predictions, create dummy
#         raise FileNotFoundError
#     print(f"Loaded {len(sample_predictions)} sample predictions.")
# except FileNotFoundError:
#     print(f"Sample predictions file {SAMPLE_PREDICTIONS_PATH} not found or ill-formatted. Creating dummy prediction data.")
#     # Create dummy image files for this demo if they don't exist from attention part
dummy_pred_image_paths = []
for i in range(3):
    path = f"dummy_pred_image_{i}.png"
    try:
        Image.new('RGB', (100, 100), color = (np.random.randint(0,255),np.random.randint(0,255),np.random.randint(0,255) )).save(path)
        dummy_pred_image_paths.append(path)
    except: pass # Fails if matplotlib backend not suitable for saving without display

if dummy_pred_image_paths:
    sample_predictions = [
        {'image_path': dummy_pred_image_paths[0], 'question': "What fruit is this?", 'ground_truth_answer': "apple", 'predicted_answer': "apple", 'is_correct': True},
        {'image_path': dummy_pred_image_paths[1], 'question': "How many bananas?", 'ground_truth_answer': "3", 'predicted_answer': "2", 'is_correct': False},
        {'image_path': dummy_pred_image_paths[2], 'question': "Is this ripe?", 'ground_truth_answer': "yes", 'predicted_answer': "yes", 'is_correct': True},
    ]
else:
    sample_predictions = []
# except Exception as e:
#     print(f"Error loading sample predictions: {e}. Skipping this section.")
#     sample_predictions = []


if sample_predictions:
    print("\n--- Sample Model Predictions ---")
    num_to_show = min(len(sample_predictions), 5)
    
    for i, sample in enumerate(sample_predictions[:num_to_show]):
        plt.figure(figsize=(6,6)) # Adjust figure size for single image display
        try:
            img = Image.open(sample['image_path'])
            plt.imshow(img)
        except FileNotFoundError:
            plt.text(0.5, 0.5, 'Image not found', horizontalalignment='center', verticalalignment='center')
            print(f"Image not found: {sample['image_path']}")
        
        title = f"Q: {sample['question']}\nGT: {sample['ground_truth_answer']} | Pred: {sample['predicted_answer']}\n"
        title += "Correct" if sample.get('is_correct', False) else "Incorrect"
        plt.title(title, fontsize=10)
        plt.axis('off')
        plt.tight_layout()
        plt.show()
        
    # Clean up dummy prediction images
    for p_path in dummy_pred_image_paths:
        if os.path.exists(p_path):
            os.remove(p_path)
else:
    print("No sample prediction data loaded. Skipping display of sample predictions.")

# %% [markdown]
# ## 6. Conclusion
#
# This notebook provided templates and examples for visualizing various aspects of your VQA model's results.
# Key visualizations include:
# - Performance metric comparisons (bar charts).
# - Qualitative attention maps (requires saved attention weights).
# - t-SNE plots for feature embeddings (requires saved features).
# - Distribution of prediction scores.
#
# Remember to replace placeholder data loading with your actual results files and adapt plotting functions as needed, especially for attention visualization which is highly model-specific.

# %% [code]
print("Results visualization notebook execution finished.")